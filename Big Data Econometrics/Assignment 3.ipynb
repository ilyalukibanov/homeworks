{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lukibanov Ilya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lalonde NSW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalinference import CausalModel\n",
    "from causalinference.utils import lalonde_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and make summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.555        5.484        6.349        7.867        1.794\n",
      "\n",
      "                       Controls (N_c=260)         Treated (N_t=185)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.827        0.379        0.843        0.365        0.044\n",
      "             X1        0.108        0.311        0.059        0.237       -0.175\n",
      "             X2       25.054        7.058       25.816        7.155        0.107\n",
      "             X3        0.154        0.361        0.189        0.393        0.094\n",
      "             X4        0.835        0.372        0.708        0.456       -0.304\n",
      "             X5       10.088        1.614       10.346        2.011        0.141\n",
      "             X6        2.107        5.688        2.096        4.887       -0.002\n",
      "             X7        0.750        0.434        0.708        0.456       -0.094\n",
      "             X8        1.267        3.103        1.532        3.219        0.084\n",
      "             X9        0.685        0.466        0.600        0.491       -0.177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y, D, X = lalonde_data()\n",
    "causal = CausalModel(Y, D, X)\n",
    "print(causal.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodegree has the highest difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated Parameters of Propensity Score\n",
      "\n",
      "                    Coef.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "     Intercept     -3.480      4.471     -0.778      0.436    -12.243      5.283\n",
      "            X6      0.034      0.051      0.667      0.505     -0.066      0.133\n",
      "            X7     -0.236      0.386     -0.611      0.541     -0.992      0.521\n",
      "            X8      0.058      0.051      1.144      0.253     -0.041      0.158\n",
      "            X9     -3.477      1.652     -2.104      0.035     -6.716     -0.238\n",
      "            X4      7.329      4.255      1.723      0.085     -1.010     15.668\n",
      "            X1     -0.653      0.385     -1.696      0.090     -1.409      0.102\n",
      "            X5      0.290      0.370      0.783      0.433     -0.435      1.015\n",
      "         X4*X5     -0.668      0.349     -1.915      0.056     -1.352      0.016\n",
      "         X6*X4     -0.130      0.057     -2.286      0.022     -0.241     -0.018\n",
      "         X9*X5      0.304      0.156      1.950      0.051     -0.002      0.609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal.est_propensity_s(lin_B=[6,7,8,9])\n",
    "print (causal.propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm also selected Nodegree, Hispanic, Education, Nodegree x  Education, Nodegree x Earnings74, and Education x Unemployment75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected cutoff: 0.131042280162\n",
      "\n",
      "Summary Statistics\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "              Y        4.543        5.501        6.237        7.587        1.694\n",
      "\n",
      "                       Controls (N_c=256)         Treated (N_t=182)             \n",
      "       Variable         Mean         S.d.         Mean         S.d.     Nor-diff\n",
      "--------------------------------------------------------------------------------\n",
      "             X0        0.828        0.378        0.841        0.367        0.034\n",
      "             X1        0.109        0.313        0.060        0.239       -0.176\n",
      "             X2       25.074        7.091       25.841        7.208        0.107\n",
      "             X3        0.156        0.364        0.187        0.391        0.081\n",
      "             X4        0.832        0.375        0.714        0.453       -0.283\n",
      "             X5       10.105        1.609       10.297        1.964        0.107\n",
      "             X6        1.675        4.435        1.795        3.876        0.029\n",
      "             X7        0.762        0.427        0.714        0.453       -0.108\n",
      "             X8        1.213        3.052        1.457        3.132        0.079\n",
      "             X9        0.691        0.463        0.604        0.490       -0.182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal.trim_s()\n",
    "print(\"Selected cutoff:\", causal.cutoff)\n",
    "print (causal.summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were dropped 260-256=4 control observations and 185-182=3 treated observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Summary\n",
      "\n",
      "              Propensity Score         Sample Size     Ave. Propensity   Outcome\n",
      "   Stratum      Min.      Max.  Controls   Treated  Controls   Treated  Raw-diff\n",
      "--------------------------------------------------------------------------------\n",
      "         1     0.131     0.379       153        67     0.327     0.332     0.788\n",
      "         2     0.380     0.483        69        63     0.435     0.443     1.587\n",
      "         3     0.487     0.852        34        52     0.596     0.619     3.044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal.stratify_s()\n",
    "print(causal.strata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: OLS\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.467      0.638      2.299      0.022      0.216      2.718\n",
      "           ATC      1.385      0.652      2.123      0.034      0.106      2.663\n",
      "           ATT      1.583      0.651      2.432      0.015      0.307      2.858\n",
      "\n",
      "Treatment Effect Estimates: Blocking\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.542      0.641      2.406      0.016      0.286      2.798\n",
      "           ATC      1.402      0.654      2.145      0.032      0.121      2.683\n",
      "           ATT      1.739      0.663      2.623      0.009      0.440      3.039\n",
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      1.400      0.888      1.576      0.115     -0.341      3.140\n",
      "           ATC      1.316      0.971      1.356      0.175     -0.587      3.219\n",
      "           ATT      1.517      0.935      1.623      0.105     -0.315      3.350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "causal.est_via_ols()\n",
    "causal.est_via_blocking()\n",
    "causal.est_via_matching(matches=2, bias_adj=True)\n",
    "print(causal.estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimates do not differ a lot: only around 10% between different estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: cramer@optilink.COM (Clayton Cramer)\\n'\n",
      " 'Subject: Re: New Study Out On Gay Percentage\\n'\n",
      " 'Organization: Optilink Corporation, Petaluma, CA\\n'\n",
      " 'Lines: 19\\n'\n",
      " '\\n'\n",
      " 'In article <C5L0v1.JCv@news.cso.uiuc.edu>, dans@uxa.cso.uiuc.edu (Dan S.) '\n",
      " 'writes:\\n'\n",
      " \"> Don't forget about the culture.  Sadly, we don't (as a society) look upon\\n\"\n",
      " '> homosexuality as normal (and as we are all too well aware, there are alot\\n'\n",
      " '> of people who condemn it).  As a result, the gay population is not '\n",
      " 'encouraged\\n'\n",
      " '> to develop \"non-promiscuous\" relationships.  In fact there are many '\n",
      " 'roadblocks\\n'\n",
      " '> put in the way of such committed relationships.  It is as if the '\n",
      " 'heterosexual\\n'\n",
      " '\\n'\n",
      " \"Such as?  Not being able to get married isn't a roadblock to a permanent\\n\"\n",
      " \"relationship.  Lack of a marriage certificate doesn't force a couple\\n\"\n",
      " 'to break up.  This is an excuse used by homosexuals because the \\n'\n",
      " 'alternative is to ask why they are so much more promiscuous than \\n'\n",
      " 'straights.\\n'\n",
      " '\\n'\n",
      " '> Dan\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '-- \\n'\n",
      " 'Clayton E. Cramer {uunet,pyramid}!optilink!cramer  My opinions, all mine!\\n'\n",
      " 'Relations between people to be by mutual consent, or not at all.\\n',\n",
      " 'From: sukenick@sci.ccny.cuny.edu (SYG)\\n'\n",
      " 'Subject: Re: AD conversion\\n'\n",
      " 'Organization: City College of New York - Science Computing Facility\\n'\n",
      " 'Lines: 33\\n'\n",
      " '\\n'\n",
      " '>> I am working a  data acquisition and analysis program to collect data\\n'\n",
      " '>> from insect sensory organs.\\n'\n",
      " '>> Another alternative is the use of the sound input port.\\n'\n",
      " '>\\n'\n",
      " '>Can you really make due with the non-existent dynamic range of an 8-bit\\n'\n",
      " '>converter, of probably dubious linearity and monotonicity, and perhaps\\n'\n",
      " '>AC-coupled as well?\\n'\n",
      " '\\n'\n",
      " \"It would depend on the requirements of the poster's data, for some\\n\"\n",
      " 'purposes 1/256 resolution (with or without calibration curve).\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Otherwise the other possibilities would be:\\n'\n",
      " '\\n'\n",
      " '1) get a digital voltameter with serial output & connect to serial\\n'\n",
      " 'port on mac, collect data with some communications program.\\n'\n",
      " '\\n'\n",
      " '2) Buy an A/D chip from Analog devices, Burr-Brown, etc, connect to\\n'\n",
      " 'a parallel to serial converter, use serial port for acquisition\\n'\n",
      " '(nah. too much soldering and trouble shooting :-)\\n'\n",
      " '\\n'\n",
      " '3) Get a board from National Instruments, Data Translation, Omega,\\n'\n",
      " 'etal.  The finest solution, but possibly the most costly.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'To the original poster:  if the signal is too large, why not\\n'\n",
      " 'use a voltage divider? Two resistors, cost very cheap...\\n'\n",
      " '-- \\n'\n",
      " '\\n'\n",
      " '\\t\\t\\t\\t\\t-george\\n'\n",
      " '\\t\\t\\t\\t\\tsukenick@sci.ccny.cuny.edu\\n'\n",
      " '\\t\\t\\t\\t\\t212-650-6028\\n',\n",
      " 'From: keegan-edward@cs.yale.edu (Edward Keegan)\\n'\n",
      " 'Subject: DEC MT 486, Adaptec SCSI, 3COMM conflict\\n'\n",
      " 'Organization: Yale University Computer Science Dept., New Haven, CT '\n",
      " '06520-2158\\n'\n",
      " 'Lines: 14\\n'\n",
      " 'Distribution: world\\n'\n",
      " 'NNTP-Posting-Host: thumper.cf.cs.yale.edu\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'I have a DEC NT 486DX33 that has an Adaptec SCSI controller, hard disk\\n'\n",
      " 'and cd-rom drive. When I add a 3COMM Ethernet card (3C503) and reboot\\n'\n",
      " 'the system I receive an error message that a boot device cannot be\\n'\n",
      " \"found. Pull the 3COMM card and reboot, everything is fine. I've moved\\n\"\n",
      " 'the controller and 3COMM card to various slots, different positions\\n'\n",
      " '(slot before the controller, slot after the controller) with the\\n'\n",
      " \"same result. DEC hasn't responded to the problem yet. Any help would\\n\"\n",
      " 'be appreciated.\\n'\n",
      " '-- \\n'\n",
      " 'Edward T. Keegan, Facility Director             E-MAIL: keegan@cs.yale.edu\\n'\n",
      " 'Yale University, Computer Science Department     PHONE: 1-203-432-1254\\n'\n",
      " '51 Prospect Street, Room 009                       FAX: 1-203-432-0593\\n'\n",
      " 'New Haven, CT 06520\\n',\n",
      " 'From: maynard@ramsey.cs.laurentian.ca (Roger Maynard)\\n'\n",
      " 'Subject: Re: Tie Breaker....(Isles and Devils)\\n'\n",
      " 'Organization: Dept. of Computer Science, Laurentian University, Sudbury, ON\\n'\n",
      " 'Lines: 18\\n'\n",
      " '\\n'\n",
      " 'In <lrw509f@rpi.edu> wangr@vccsouth22.its.rpi.edu ( Rex Wang ) writes:\\n'\n",
      " '\\n'\n",
      " '>I might not be great in Math, but tell me how can two teams ahve the same '\n",
      " 'points\\n'\n",
      " \">with different record??? Man...retard!!!!!! Can't believe people actually \"\n",
      " 'put\\n'\n",
      " '>win as first in a tie breaker......\\n'\n",
      " '\\n'\n",
      " \"Well I don't see any smileys here.  I am trying to figure out if the poster\\n\"\n",
      " \"is a dog or a wordprocessor.  Couldn't be neither.  Both are smarter than\\n\"\n",
      " 'this.\\n'\n",
      " '\\n'\n",
      " '\"I might not be great in Math\"\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '-- \\n'\n",
      " '\\n'\n",
      " 'cordially, as always,                      maynard@ramsey.cs.laurentian.ca \\n'\n",
      " '                                           \"So many morons...\\n'\n",
      " 'rm                                                   ...and so little '\n",
      " 'time.\" \\n']\n"
     ]
    }
   ],
   "source": [
    "pprint(newsgroups_train.data[110:114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting into bags of words (creating dictionary from train data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer()\n",
    "c_vec.fit(newsgroups_train.data)\n",
    "text_data_num = c_vec.transform(newsgroups_train.data)\n",
    "text_data_test = c_vec.transform(newsgroups_test.data)\n",
    "text_data_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, dictionary has the lenght of 130107 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVD to reduce number of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98811968477911039"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 30\n",
    "svd = TruncatedSVD(n_components = K)\n",
    "svd.fit(text_data_num)\n",
    "transformed_data = svd.transform(text_data_num)\n",
    "transrormed_test = svd.transform(text_data_test)\n",
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use Multinomial Naive Bayes because it requires only positive numbers (and during dimensionality reduction some of the new variable will be negative); however, it has a great performance without dimensionality reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924518295917\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB().fit(text_data_num, newsgroups_train.target)\n",
    "print(metrics.accuracy_score(newsgroups_train.target, mnb.predict(text_data_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772835900159\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_test.target, mnb.predict(text_data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-sample 92% are right classified, and 77% out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use favourite tool of empirical IO and Marketing - multinomial logit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(transformed_data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.403482411172\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_train.target, mul_lr.predict(transformed_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.353823685608\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_test.target, mul_lr.predict(transrormed_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40% in-sample - not so good, but 35% out of sample - quite impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Gaussian naive bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB().fit(transformed_data, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.129397207\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_train.target, gnb.predict(transformed_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.128518321827\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_test.target, gnb.predict(transrormed_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimpressive performance, but almost no difference between in and out of sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "results = np.zeros((10,2))\n",
    "results[k,0] = metrics.accuracy_score(newsgroups_train.target, mul_lr.predict(transformed_data))\n",
    "results[k,1] = metrics.accuracy_score(newsgroups_test.target, mul_lr.predict(transrormed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "0.483825349125\n",
      "0.41409984068\n",
      "50\n",
      "0.526073890755\n",
      "0.450876261285\n",
      "60\n",
      "0.560809616404\n",
      "0.482873074881\n",
      "70\n",
      "0.592186671381\n",
      "0.509691980882\n",
      "80\n",
      "0.635142301573\n",
      "0.538767923526\n",
      "90\n",
      "0.663249071946\n",
      "0.566117896973\n",
      "100\n",
      "0.690737139827\n",
      "0.583244822092\n"
     ]
    }
   ],
   "source": [
    "for i in [40,50,60,70,80,90,100]:\n",
    "    k = k + 1\n",
    "    svd = TruncatedSVD(n_components = i)\n",
    "    svd.fit(text_data_num)\n",
    "    transformed_data = svd.transform(text_data_num)\n",
    "    transrormed_test = svd.transform(text_data_test)\n",
    "    mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(transformed_data, newsgroups_train.target)\n",
    "    print(i)\n",
    "    results[k,0] = metrics.accuracy_score(newsgroups_train.target, mul_lr.predict(transformed_data))\n",
    "    results[k,1] = metrics.accuracy_score(newsgroups_test.target, mul_lr.predict(transrormed_test))\n",
    "    print(results[k,0])\n",
    "    print(results[k,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is increasing with number of dimensions. If we use multinomial logit for the whole sample, we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(text_data_num, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99991161393\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_train.target, mul_lr.predict(text_data_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789298990972\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(newsgroups_test.target, mul_lr.predict(text_data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, out of sample accuracy is 78.9% which is even higher than MultinomialNB. However, it takes a lot of time to compute. The best K is the whole sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
